{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This seminar will help you to do your homework on creating a panorama view. Let's install some useful libraries"
      ],
      "metadata": {
        "id": "Y0TOHpkst_tY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip input.zip"
      ],
      "metadata": {
        "id": "-ghUs5ErvS5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "%pip install opencv-python matplotlib numpy ipython-autotime"
      ],
      "metadata": {
        "id": "1yHlR9hp44mz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import glob\n",
        "import itertools\n",
        "import math\n",
        "import os.path as osp\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "\n",
        "# matplotlib.use('TkAgg')  # Uncomment for macOS\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (20.0, 16.0)\n",
        "plt.rcParams['image.interpolation'] = 'bilinear'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "id": "FiuTmSd_44m0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Panorama stitching"
      ],
      "metadata": {
        "collapsed": false,
        "id": "MGxV3YQJ44m1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://media.springernature.com/lw685/springer-static/image/prt%3A978-0-387-31439-6%2F9/MediaObjects/978-0-387-31439-6_9_Part_Fig1-13_HTML.jpg)\n",
        "\n",
        "The seminar consists of two parts. The first part is dedicated to panorama stitching, and the second part covers camera calibration. Camera calibration is not mandatory, but there is no camera without distortion, as a result of which straight lines will not appear straight. This is particularly evident in wide-angle cameras, so calibration is essential in such cases.\n",
        "\n",
        "\n",
        "\n",
        "Example of distortion create by [demo](https://jywarren.github.io/fisheyegl/example/#a=3.276&b=2.223&Fx=2&Fy=0.26&scale=1.243&x=0&y=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "SFStRv-88u_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Images"
      ],
      "metadata": {
        "collapsed": false,
        "id": "UW2AxW2_44m1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading images. Here's an example of three photos taken at home that we want to stitch into a panorama. In your homework, you have five, so you'll have to put in some work :)"
      ],
      "metadata": {
        "id": "5Q-6MBbP8_dw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eXZjlo4Y44m1"
      },
      "outputs": [],
      "source": [
        "input_dir = 'input'\n",
        "ipaths = sorted(glob.glob(osp.join(input_dir, 'img_*.jpg')))\n",
        "n_imgs = len(ipaths)\n",
        "\n",
        "images_info = dict()\n",
        "for i_num, ipath in enumerate(ipaths):\n",
        "    img = plt.imread(ipath)\n",
        "    images_info[ipath] = {'img': img}\n",
        "    plt.subplot(1, n_imgs, i_num + 1)\n",
        "    plt.title(osp.basename(ipath), fontsize=20)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(images_info[ipath]['img'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keypoints detection + descriptors\n",
        "\n",
        "To stitch the panorama, we need to obtain the homography matrix. We can obtain the homography matrix using the RANSAC method. For generating keypoints, we will use SIFT.\n",
        "\n",
        "It's worth noting that SIFT is performed on grayscale images. This is done to speed up the process, as SIFT on RGB does not significantly improve the quality.\n",
        "\n",
        "By varying the SIFT parameters, you can obtain a different number of keypoints and types. Therefore, try experimenting with these parameters.\n",
        "\n",
        "![](https://www.codeproject.com/KB/recipes/619039/SIFT.JPG)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "XeTYZonO44m2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def keypoints_detection_sift(input_img):\n",
        "    gray = cv2.cvtColor(input_img, cv2.COLOR_RGB2GRAY)\n",
        "    sift = cv2.SIFT_create()\n",
        "    kps, dscrs = sift.detectAndCompute(gray, None)\n",
        "    return kps, dscrs"
      ],
      "metadata": {
        "id": "RuJOa0_q44m2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can visualize keypoint descriptors from SIFT and can visualize them."
      ],
      "metadata": {
        "id": "w0vEoxfi98Sg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "for i_num, ipath in enumerate(images_info):\n",
        "    img = images_info[ipath]['img']\n",
        "    keypoints, descriptors = keypoints_detection_sift(img)\n",
        "    images_info[ipath]['keypoints'] = keypoints\n",
        "    images_info[ipath]['descriptors'] = descriptors\n",
        "    plt.subplot(1, n_imgs, i_num + 1)\n",
        "    plt.title(osp.basename(ipaths[i_num]), fontsize=20)\n",
        "    plt.imshow(cv2.drawKeypoints(\n",
        "        img, keypoints, None,\n",
        "        flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n",
        "    ))\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NtXwupmr44m3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are plenty of keypoints in the image, and it can be noticed that they come in different sizes, meaning they come from different octaves. It's also noticeable that there are too many of them, and we would like to filter them out :)\n",
        "\n",
        "**HINT:** It's very pleasant to detect keypoints in the text; they will turn out to be corners that match well with each other. Sort the keypoints by size.\n",
        "\n",
        "*Off-topic:* We can speed up the calculation using int8 cast, but it's not essential for our task."
      ],
      "metadata": {
        "id": "QWobhnyW-H_c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "keypoints, descriptors = images_info[ipaths[0]]['keypoints'], \\\n",
        "    images_info[ipaths[0]]['descriptors']\n",
        "keypoint = sorted(keypoints, key=lambda x: x.size, reverse=True)[0]\n",
        "for field in dir(keypoint):\n",
        "    if not field.startswith('_'):\n",
        "        print(f'{field:10} {getattr(keypoint, field)}')"
      ],
      "metadata": {
        "id": "Qp94uWiR44m4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "_ = plt.figure(figsize=(10, 8))\n",
        "plt.imshow(cv2.drawKeypoints(\n",
        "    images_info[ipaths[0]]['img'], [keypoint, ], None,\n",
        "    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
        ")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YBqznrgR44m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look how good panorama should looks like. (Not good ðŸ˜ž)"
      ],
      "metadata": {
        "id": "mUJoWtCw3-VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install kornia kornia_rs # kornia_rs for Linux"
      ],
      "metadata": {
        "id": "W7L1sYyT4yYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kornia as K\n",
        "from kornia.core import Tensor\n",
        "from kornia.contrib import ImageStitcher\n",
        "import kornia.feature as KF\n",
        "import torch\n",
        "\n",
        "def inference(file_1, file_2):\n",
        "    img_1: Tensor = K.io.load_image(file_1, K.io.ImageLoadType.RGB32)\n",
        "    img_1 = img_1[None]  # 1xCxHxW / fp32 / [0, 1]\n",
        "    img_2: Tensor = K.io.load_image(file_2, K.io.ImageLoadType.RGB32)\n",
        "    img_2 = img_2[None]  # 1xCxHxW / fp32 / [0, 1]\n",
        "\n",
        "    IS = ImageStitcher(KF.LoFTR(pretrained='indoor'), estimator='ransac')\n",
        "    with torch.no_grad():\n",
        "        result = IS(img_1, img_2)\n",
        "\n",
        "    return K.tensor_to_image(result[0])"
      ],
      "metadata": {
        "id": "fTzczW6tELVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "panorama = inference('/content/input/img_00.jpg', '/content/input/img_01.jpg')"
      ],
      "metadata": {
        "id": "rRMYWx2946MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(panorama)"
      ],
      "metadata": {
        "id": "DBu-h11i6h8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of panorama with Kornia... Not good"
      ],
      "metadata": {
        "id": "gs_4hOlY7H8Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Keypoints matching"
      ],
      "metadata": {
        "collapsed": false,
        "id": "krY_xomb44m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bruteforce + Cross-check"
      ],
      "metadata": {
        "collapsed": false,
        "id": "GZeJ7_iH44m4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For matching, we will use a simple brute-force algorithm. We will calculate the Euclidean distance and for each keypoint from the first image, we will find the corresponding keypoint on the second image.\n",
        "\n",
        "Additionally, we can implement cross-checking. If we run the keypoint matching procedure from the second image to the first, not all points will be mapped to the exact same points. Therefore, we can check the reliability of some keypoints."
      ],
      "metadata": {
        "id": "1ITeGXR-jaq_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def keypoints_matching_cross_check(dscrs1, dscrs2):\n",
        "    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "    cross_matches = bf.match(dscrs1, dscrs2)\n",
        "    return cross_matches"
      ],
      "metadata": {
        "id": "74CaX6lH44m4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i_num, ipath in enumerate(images_info):\n",
        "    print(images_info[ipath]['descriptors'])"
      ],
      "metadata": {
        "id": "VV3KHH2djPW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "matches = keypoints_matching_cross_check(\n",
        "    images_info[ipaths[0]]['descriptors'],\n",
        "    images_info[ipaths[1]]['descriptors']\n",
        ")"
      ],
      "metadata": {
        "id": "UKVso-zT44m5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's see how the points have been mapped to each other."
      ],
      "metadata": {
        "id": "LHXdSZRJj1nc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "img_matches = cv2.drawMatches(\n",
        "    images_info[ipaths[0]]['img'],\n",
        "    images_info[ipaths[0]]['keypoints'],\n",
        "    images_info[ipaths[1]]['img'],\n",
        "    images_info[ipaths[1]]['keypoints'],\n",
        "    matches,\n",
        "    None,  # output image\n",
        "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        ")\n",
        "plt.imshow(img_matches)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5v_VNoYq44m5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a couple of key points on the straight lines, and we don't like them because the differences between the points on the line are not very significant.\n",
        "\n",
        "There are also a huge number of points on the keyboard keys. Since the keys are very similar to each other, it will be difficult to organize a correct matching on them. Therefore, we will filter them using the Ratio test."
      ],
      "metadata": {
        "id": "NpFxPk7_xZmX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN + Ratio test\n",
        "\n",
        "By reducing the number of points based on distance, we ensure that there are no points in the vicinity that have similar points. You can play around with this parameter, decreasing or increasing it as per your requirement."
      ],
      "metadata": {
        "collapsed": false,
        "id": "o3RGEvs144m5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def keypoints_matching_knn(dscrs1, dscrs2):\n",
        "    bf = cv2.BFMatcher()\n",
        "    tic = time.time_ns()\n",
        "    knn_matches = bf.knnMatch(dscrs1, dscrs2, k=2)\n",
        "    toc = time.time_ns()\n",
        "    print(f'Matching time: {(toc - tic) / 10e6 :.5f} ms')\n",
        "    good_matches = []\n",
        "    for neighbour_1, neighbour_2 in knn_matches:\n",
        "        if neighbour_1.distance < 0.75 * neighbour_2.distance:\n",
        "            good_matches.append(neighbour_1)\n",
        "    return good_matches"
      ],
      "metadata": {
        "id": "klF_Fb0E44m5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "matches = keypoints_matching_knn(\n",
        "    images_info[ipaths[0]]['descriptors'],\n",
        "    images_info[ipaths[1]]['descriptors']\n",
        ")"
      ],
      "metadata": {
        "id": "3VOcI0Lf44m6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "img_matches = cv2.drawMatches(\n",
        "    images_info[ipaths[0]]['img'],\n",
        "    images_info[ipaths[0]]['keypoints'],\n",
        "    images_info[ipaths[1]]['img'],\n",
        "    images_info[ipaths[1]]['keypoints'],\n",
        "    matches,\n",
        "    None,  # output image\n",
        "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        ")\n",
        "plt.imshow(img_matches)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RUMFgvzH44m6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FLANN"
      ],
      "metadata": {
        "collapsed": false,
        "id": "louP8mqK44m6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The search for matches in our image is quite fast, but it's still worth mentioning the methods for fast nearest neighbor search. For example, when we are performing face matching with a database of 150 million faces, using the classical nearest neighbor search is not advantageous. Therefore, FLANN, which is based on KD-trees, can be applied."
      ],
      "metadata": {
        "id": "UT5WD5x_k9ev"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def keypoints_matching_flann(dscrs1, dscrs2):\n",
        "    index_params = dict(\n",
        "        algorithm=1,  # FLANN_INDEX_KDTREE\n",
        "        trees=5\n",
        "    )\n",
        "    search_params = dict(checks=20)\n",
        "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
        "    tic = time.time_ns()\n",
        "    flann_matches = flann.knnMatch(dscrs1, dscrs2, k=2)\n",
        "    toc = time.time_ns()\n",
        "    print(f'Matching time: {(toc - tic) / 10e6 :.5f} ms')\n",
        "    good_matches = []\n",
        "    for neighbour_1, neighbour_2 in flann_matches:\n",
        "        if neighbour_1.distance < 0.75 * neighbour_2.distance:\n",
        "            good_matches.append(neighbour_1)\n",
        "    return good_matches"
      ],
      "metadata": {
        "id": "8NZ_ycH444m6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "matches = keypoints_matching_flann(\n",
        "    images_info[ipaths[0]]['descriptors'],\n",
        "    images_info[ipaths[1]]['descriptors']\n",
        ")"
      ],
      "metadata": {
        "id": "YOkg5IuS44m7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "img_matches = cv2.drawMatches(\n",
        "    images_info[ipaths[0]]['img'],\n",
        "    images_info[ipaths[0]]['keypoints'],\n",
        "    images_info[ipaths[1]]['img'],\n",
        "    images_info[ipaths[1]]['keypoints'],\n",
        "    matches,\n",
        "    None,  # output image\n",
        "    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        ")\n",
        "plt.imshow(img_matches)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NLuzRAvg44m7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we use our images, then the time difference is small. To let you feel how big the difference is, let's increase the number of points to see how much the matching differs in efficiency.\n",
        "\n",
        "It's not very fair for the KD tree because the points will be duplicated in the image, but it helps to feel the difference excellently. Overall, KNN is not that slow, so it is not recommended to use FLANN, as there are few images."
      ],
      "metadata": {
        "id": "lyugfpGkl60A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "print(f'Descriptors shape on the image_0: {images_info[ipaths[0]][\"descriptors\"].shape}')\n",
        "print(f'Descriptors shape on the image_1: {images_info[ipaths[1]][\"descriptors\"].shape}')\n",
        "mock_descriptors_1 = np.repeat(images_info[ipaths[0]]['descriptors'], 100, axis=0)\n",
        "mock_descriptors_2 = np.repeat(images_info[ipaths[1]]['descriptors'], 100, axis=0)\n",
        "print(f'Descriptors shape on the mock_image_0: {mock_descriptors_1.shape}')\n",
        "print(f'Descriptors shape on the mock_image_1: {mock_descriptors_2.shape}')"
      ],
      "metadata": {
        "id": "SMq3hQal44m7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "_ = keypoints_matching_knn(mock_descriptors_1, mock_descriptors_2)"
      ],
      "metadata": {
        "id": "88msd6xt44m8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "_ = keypoints_matching_flann(mock_descriptors_1, mock_descriptors_2)"
      ],
      "metadata": {
        "id": "jSfxDdda44m8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find matches for images"
      ],
      "metadata": {
        "collapsed": false,
        "id": "1n-V24ja44m8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "matches_images = dict()\n",
        "for ipath_0, ipath_1 in itertools.permutations(ipaths, 2):\n",
        "    matches_pair = keypoints_matching_knn(\n",
        "        images_info[ipath_0]['descriptors'],\n",
        "        images_info[ipath_1]['descriptors']\n",
        "    )\n",
        "    matches_images[(ipath_0, ipath_1)] = matches_pair"
      ],
      "metadata": {
        "id": "rqXddF5B44m8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homography"
      ],
      "metadata": {
        "collapsed": false,
        "id": "DxtOM6FN44m9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a homography matrix, we will use the RANSAC method. RANSAC handles outliers well. (You need 4 points for homography, you can take more through SVD).\n",
        "\n",
        "One of the important parameters you can play with is the threshold."
      ],
      "metadata": {
        "id": "XVORSywdhAnu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "homographies = dict()\n",
        "for (ipath_0, ipath_1), matches in matches_images.items():\n",
        "    src_pts = np.float32(\n",
        "        [images_info[ipath_0]['keypoints'][m.queryIdx].pt for m in matches]\n",
        "    ).reshape(-1, 1, 2)\n",
        "    dst_pts = np.float32(\n",
        "        [images_info[ipath_1]['keypoints'][m.trainIdx].pt for m in matches]\n",
        "    ).reshape(-1, 1, 2)\n",
        "    M, mask = cv2.findHomography(\n",
        "        src_pts,\n",
        "        dst_pts,\n",
        "        cv2.RANSAC,\n",
        "        ransacReprojThreshold=.5,\n",
        "        maxIters=20000,\n",
        "        confidence=0.995\n",
        "    )\n",
        "    homographies[ipath_0, ipath_1] = (M, mask.ravel().tolist())"
      ],
      "metadata": {
        "id": "qBey5lay44m9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "M"
      ],
      "metadata": {
        "id": "GmCWGglGiT6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we have constructed the homography matrix, we can observe how one image is projected onto another.  "
      ],
      "metadata": {
        "id": "ZPPLBPn7iq9W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "for ipath_0, ipath_1 in itertools.permutations(ipaths, 2):\n",
        "    matches = matches_images[ipath_0, ipath_1]\n",
        "    M, matches_mask = homographies[ipath_0, ipath_1]\n",
        "    img_0, img_1_polylines = images_info[ipath_0]['img'].copy(), \\\n",
        "        images_info[ipath_1]['img'].copy()\n",
        "    rows_0, cols_0 = img_0.shape[:2]\n",
        "    rows_1, cols_1 = img_1_polylines.shape[:2]\n",
        "    pts = np.float32(\n",
        "        [[0, 0], [0, rows_0], [cols_0, rows_0], [cols_0, 0]]).reshape(-1, 1, 2)\n",
        "    dst = cv2.perspectiveTransform(pts, M)\n",
        "    img_1_polylines = cv2.polylines(\n",
        "        img_1_polylines, [np.int32(dst)], True, 255, 3, cv2.LINE_AA\n",
        "    )\n",
        "    img_matches = cv2.drawMatches(\n",
        "        img_0,\n",
        "        images_info[ipath_0]['keypoints'],\n",
        "        img_1_polylines,\n",
        "        images_info[ipath_1]['keypoints'],\n",
        "        matches,\n",
        "        None,  # output image\n",
        "        matchesMask=matches_mask,\n",
        "        flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n",
        "    )\n",
        "    plt.title(\n",
        "        f'Pair of images: {ipath_0} and {ipath_1} ({sum(matches_mask)} matches)',\n",
        "        fontsize=20\n",
        "    )\n",
        "    plt.imshow(img_matches)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QvGiYCUj44m9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stitching images"
      ],
      "metadata": {
        "collapsed": false,
        "id": "pJC9onks44m9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "M, _ = homographies[ipaths[1], ipaths[0]]\n",
        "img_0, img_1 = images_info[ipaths[0]]['img'], images_info[ipaths[1]]['img']\n",
        "img_0_1 = cv2.warpPerspective(\n",
        "    img_1, M, (img_0.shape[1] + img_1.shape[1], img_1.shape[0])\n",
        ")\n",
        "img_0_1[0:img_0.shape[0], 0:img_0.shape[1]] = img_0\n",
        "\n",
        "# Some postprocessing\n",
        "img_0_1_mask = np.where(img_0_1.sum(axis=2) != 0, 1, 0).astype('uint8')\n",
        "kernel = np.ones((5, 5), np.uint8)\n",
        "img_0_1_mask = cv2.morphologyEx(img_0_1_mask, cv2.MORPH_CLOSE, kernel)\n",
        "img_0_1_mask = cv2.erode(img_0_1_mask, kernel, iterations=1)\n",
        "img_0_1_mask = np.dstack([img_0_1_mask, ] * 3)"
      ],
      "metadata": {
        "id": "rDj61sgU44m9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.imshow(img_0_1)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "19KDUgHz44m-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "M, _ = homographies[ipaths[2], ipaths[0]]\n",
        "img_2 = images_info[ipaths[2]]['img']\n",
        "img_2_pano = cv2.warpPerspective(\n",
        "    img_2, M, (img_0.shape[1] + img_1.shape[1], img_1.shape[0])\n",
        ")\n",
        "panorama = np.where(img_0_1_mask != 0, img_0_1, img_2_pano)"
      ],
      "metadata": {
        "id": "r-YY7NLV44m-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.imshow(panorama)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iPMk41X144m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, the first image is being stitched with the second one, so the quality is slightly worse, but you can stitch the first image with the second one and the quality will increase."
      ],
      "metadata": {
        "id": "NmsZv7mZz5Je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hints\n",
        "\n",
        "To achieve good quality, try varying the parameters mentioned above. Iterate through this process to obtain excellent images.\n",
        "\n",
        "Additionally, you can implement a more sophisticated image blending by normalizing the image colors or blending with consideration of the alpha channel.\n",
        "\n",
        "This will help you address issues arising from the warp perspective.\n"
      ],
      "metadata": {
        "id": "SitTp4jzkWna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Camera calibration"
      ],
      "metadata": {
        "collapsed": false,
        "id": "vn7WGppq44m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better understand the camera parameters one can try these demos"
      ],
      "metadata": {
        "id": "4GMW7v1X-Iky"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extrinsic parameters demo:\n",
        "\n",
        "https://ksimek.github.io/2012/08/22/extrinsic/#:~:text=%3D%20%2DRC.-,Try%20it%20out!,-Below%20is%20an\n",
        "\n",
        "Intrinsic parameters demo:\n",
        "\n",
        "https://ksimek.github.io/2013/08/13/intrinsic/#:~:text=in%20image%20space.-,Demo,-The%20demo%20below\n",
        "\n",
        "\n",
        "To better understand distortion, check out this.\n",
        "\n",
        "https://www.tangramvision.com/blog/camera-modeling-exploring-distortion-and-distortion-models-part-i"
      ],
      "metadata": {
        "id": "38s-51Yn-yAv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For camera calibration, we will use a special pattern called a chessboard. Its parameters are well known to us, so calibrating the camera with its help will be easier than through the pipeline we used for panorama construction. The image of the chessboard can be found in the OpenCV distribution on [GitHub](https://github.com/opencv/opencv/blob/4.x/samples/data/chessboard.png). It needs to be printed and photographed from different angles and distances to obtain an adequate number of images for camera calibration. The total number of images should be no less than 10."
      ],
      "metadata": {
        "collapsed": false,
        "id": "5yKiuXzI44m-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "input_dir = 'input'\n",
        "chessboard_ipaths = sorted(glob.glob(osp.join(input_dir, 'chessboard_*.jpg')))\n",
        "n_chessboard_imgs = len(chessboard_ipaths)\n",
        "print(f'Number of chessboard images: {n_chessboard_imgs}')"
      ],
      "metadata": {
        "id": "BoXU_pDi44nN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "On each image, it is necessary to find the corners of the chessboard. For this purpose, the `cv2.findChessboardCorners` function is used. This function provides correspondence between the corners of the chessboard in the real world (object points) and the points on the image (image points).\n",
        "\n",
        "After finding the corners, their location can be refined using the `cv2.cornerSubPix` function. You can use this function if you find the calibration quality obtained without it to be insufficient."
      ],
      "metadata": {
        "collapsed": false,
        "id": "pkdBQocC44nN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(8,5,0)\n",
        "objp = np.zeros((6 * 9, 3), np.float32)\n",
        "objp[:, :2] = np.mgrid[0:9, 0:6].T.reshape(-1, 2)\n",
        "# Arrays to store object points and image points from all the images\n",
        "obj_points = []  # 3d points in real world space\n",
        "img_points = []  # 2d points in image plane."
      ],
      "metadata": {
        "id": "OjO6NRjO44nN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "_ = plt.figure(figsize=(16, 36))\n",
        "for i_num, ipath in enumerate(chessboard_ipaths):\n",
        "    img = cv2.cvtColor(cv2.imread(ipath), cv2.COLOR_BGR2RGB)\n",
        "    img_grayscale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "    result, corners = cv2.findChessboardCorners(img_grayscale, (9, 6), None)\n",
        "    if result:\n",
        "        obj_points.append(objp)\n",
        "        img_points.append(corners)\n",
        "\n",
        "        cv2.drawChessboardCorners(img, (9, 6), corners, result)\n",
        "        plt.subplot(math.ceil(n_chessboard_imgs / 3), 3, i_num + 1)\n",
        "        plt.title(osp.basename(ipath), fontsize=12)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(img)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tu-4QaO444nN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having correspondences between the corners of the chessboard in the real world and the points on the image, you can calibrate the camera. For this purpose, the `cv2.calibrateCamera` function is used. As a result of calibration, we will obtain the camera matrix and distortion coefficients."
      ],
      "metadata": {
        "collapsed": false,
        "id": "9rtfE5zx44nO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "img = cv2.cvtColor(cv2.imread(chessboard_ipaths[-1]), cv2.COLOR_BGR2RGB)\n",
        "img_grayscale = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
        "result, mtx, dist, r_vecs, t_vecs = cv2.calibrateCamera(\n",
        "    objectPoints=obj_points,\n",
        "    imagePoints=img_points,\n",
        "    imageSize=img_grayscale.shape[::-1],\n",
        "    cameraMatrix=None,\n",
        "    distCoeffs=None\n",
        ")\n",
        "print(f'Camera matrix:\\n{mtx}')\n",
        "print(f'Distortion coefficients:\\n{dist}')"
      ],
      "metadata": {
        "id": "r-NF15XD44nO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "np.savetxt('mtx.txt', mtx)\n",
        "np.savetxt('dist.txt', dist)"
      ],
      "metadata": {
        "id": "vLmFpn0p44nO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These parameters can be used to correct images obtained with this camera. The `cv2.undistort` function is used for this purpose. Additionally, one can use the `cv2.getOptimalNewCameraMatrix` function to obtain a new camera matrix. This new matrix will contain only those pixels that carry information about the image."
      ],
      "metadata": {
        "collapsed": false,
        "id": "TnxCRdhM44nO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "rows, cols = img.shape[:2]\n",
        "new_camera_mtx, roi = cv2.getOptimalNewCameraMatrix(\n",
        "    cameraMatrix=mtx,\n",
        "    distCoeffs=dist,\n",
        "    imageSize=(cols, rows),\n",
        "    alpha=1,\n",
        "    newImgSize=(cols, rows)\n",
        ")\n",
        "print(f'New camera matrix:\\n{new_camera_mtx}')\n",
        "print(f'Region of interest:\\n{roi}')"
      ],
      "metadata": {
        "id": "UftNNxRd44nP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "dst = cv2.undistort(\n",
        "    src=img,\n",
        "    cameraMatrix=mtx,\n",
        "    distCoeffs=dist,\n",
        "    dst=None,\n",
        "    newCameraMatrix=new_camera_mtx\n",
        ")\n",
        "\n",
        "plt.imshow(dst)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cFVhNSTk44nP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "roi_x, roi_y, roi_w, roi_h = roi\n",
        "dst = dst[roi_y:roi_y + roi_h, roi_x:roi_x + roi_w]\n",
        "plt.imshow(dst)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y5ufxya944nP"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}